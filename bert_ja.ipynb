{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.6.1\n",
    "!pip install fugashi\n",
    "!pip install unidic_lite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "get_feature = pipeline('feature-extraction', \n",
    "                        model='cl-tohoku/bert-base-japanese-v2', \n",
    "                        tokenizer='cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_word = \"ワクチン\"\n",
    "hidden_state = get_feature(sample_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(hidden_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer(sample_word)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_vector(sample_text):\n",
    "      hidden_state = get_feature(sample_text, padding=True, truncation=True, max_length=512)\n",
    "  cls_vec = np.array(hidden_state)[0, 0]\n",
    "  return cls_vec\n",
    "\n",
    "ml = moderna_text.split(\"\\n\")\n",
    "pfzl = pfizer_text.split(\"\\n\")\n",
    "ml = [text for text in ml if text != '']\n",
    "pfzl = [text for text in pfzl if text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = moderna_text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfzl = pfizer_text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = [text for text in ml if text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pfzl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfzl = [text for text in pfzl if text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pfzl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderna_vecs = np.array([get_cls_vector(text) for text in ml])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer_vecs = np.array([get_cls_vector(text) for text in pfzl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderna_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pfizer_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pfizer_dics = {}\n",
    "for text in pfzl:\n",
    "  for token in tokenizer.tokenize(text):\n",
    "    if len(token) > 1:\n",
    "      if token not in pfizer_dics:\n",
    "        pfizer_dics[token] = 1\n",
    "      else:\n",
    "        pfizer_dics[token] += 1\n",
    "sorted(pfizer_dics.items(), key=lambda x:-x[1])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dics = {}\n",
    "for text in ml:\n",
    "  for token in tokenizer.tokenize(text):\n",
    "    if len(token) > 1:\n",
    "      if token not in ml_dics:\n",
    "        ml_dics[token] = 1\n",
    "      else:\n",
    "        ml_dics[token] += 1\n",
    "sorted(ml_dics.items(), key=lambda x:-x[1])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "moderna_vecs_reduced = tsne.fit_transform(moderna_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 7))\n",
    "plt.scatter(moderna_vecs_reduced[:, 0], moderna_vecs_reduced[:, 1],  c=[i for i in range(len(ml))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "pfizer_vecs_reduced = tsne.fit_transform(pfizer_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 7))\n",
    "plt.scatter(pfizer_vecs_reduced[:, 0], pfizer_vecs_reduced[:, 1], c=[i for i in range(len(pfzl))])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
